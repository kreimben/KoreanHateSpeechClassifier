{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Korean Hate Speech Classifier\n",
    "## Written by: [Jehwan Kim](github.com/kreimben)\n",
    "## Date: 19th Feb 2024\n",
    "## Referenced Paper: \n",
    "* [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1408.5882)\n",
    "* [Cyclical Learning Rates for Training Neural Networks](https://arxiv.org/abs/1506.01186)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b85a589180c7304"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load the word2vec model first,"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76142ca29aea3a03"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import lightning as L\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95ba83ff602bf2ae",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### And then, load labeled data using pandas"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc778d92bb73c7c1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dev_df = pd.read_csv('./labeled/dev.tsv', sep='\\t')\n",
    "train_df = pd.read_csv('./labeled/train.tsv', sep='\\t')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "483cddf9b1ef8ff5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_df.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "957b5d894af019aa",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dev_df.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6998e4faa8a15643",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### In `hate` column, `offensive`, `none`, `hate`.\n",
    "### In `contain_gender_bias` column, `True`, `False`.\n",
    "### In `bias` column, `none`, `gender`, `others`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f3074856f112f7c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# combine train and dev data.\n",
    "df = pd.concat([dev_df, train_df], ignore_index=True)\n",
    "df.sample(5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57a62ec519832594",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.hate.value_counts(), dev_df.contain_gender_bias.value_counts(), dev_df.bias.value_counts()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "adc2696d1f6809b2",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load words data and tokeniser from past project."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48640b0cad2eddfe"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('tokenizer.pkl', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "with open('words.pkl', 'rb') as handle:\n",
    "    words = pickle.load(handle)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c309aefe176e427a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokenizer.tokenize('혼전임신은 미리 조심하지 못한 여자 잘못이 크다')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32c7deac9d238855",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from utils.stopwords import STOP_WORDS\n",
    "\n",
    "\n",
    "def tokenize(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    preprocess = lambda x: [w for w in x if w not in STOP_WORDS]\n",
    "    return preprocess(tokens)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d3dbea551c4ae89",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df['tokens'] = df['comments'].apply(tokenize)\n",
    "# 공격적인(offensive) 댓글 또한 혐오 데이터 셋으로 분류함.\n",
    "df['hate'] = df['hate'].replace(['none', 'offensive', 'hate'], [0, 1, 1])\n",
    "df['contain_gender_bias'] = df['contain_gender_bias'].replace([True, False], [1, 0])\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da709590362b1e96",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "vocab_size = len(words.keys())\n",
    "vocab_size"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b281c2177fdf376",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Vectorise"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7ef7501882fb09f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with open('vectorizer.pkl', 'rb') as handle:\n",
    "    vectorizer = pickle.load(handle)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b6238857638d8c2",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Encoding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9a0973672e14a64"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df['encoding'] = df['comments'].apply(vectorizer.encode_a_doc_to_list)\n",
    "df.encoding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae2f4030b420969e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_data = df['encoding']\n",
    "y_data = df['hate']\n",
    "print(y_data.value_counts())\n",
    "len(X_data), len(y_data), len(X_data) == len(y_data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "abc1b33996c85487",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=.1, random_state=42, stratify=y_data)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=.1, random_state=42, stratify=y_train)\n",
    "\n",
    "y_train.value_counts()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aea73a715225d891",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print('공격적인(offensive) 댓글 또한 혐오 데이터 셋으로 분류함.')\n",
    "print('--------훈련 데이터의 비율-----------')\n",
    "print(f'혐오 댓글 = {round(y_train.value_counts()[1] / len(y_train) * 100, 3)}%')\n",
    "print(f'일반 댓글 = {round(y_train.value_counts()[0] / len(y_train) * 100, 3)}%')\n",
    "print('--------검증 데이터의 비율-----------')\n",
    "print(f'혐오 댓글 = {round(y_valid.value_counts()[1] / len(y_valid) * 100, 3)}%')\n",
    "print(f'일반 댓글 = {round(y_valid.value_counts()[0] / len(y_valid) * 100, 3)}%')\n",
    "print('--------테스트 데이터의 비율-----------')\n",
    "print(f'혐오 댓글 = {round(y_test.value_counts()[1] / len(y_test) * 100, 3)}%')\n",
    "print(f'일반 댓글 = {round(y_test.value_counts()[0] / len(y_test) * 100, 3)}%')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "636c107f5616018a",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Padding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc3cc8cad19abafc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print('댓글의 최대 길이 :', max(len(review) for review in X_train))\n",
    "print('댓글의 평균 길이 :', sum(map(len, X_train)) / len(X_train))\n",
    "plt.hist([len(review) for review in X_train], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9771b28e6e289d0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "max_len = max(len(review) for review in X_train)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e299ce12a3f55b8",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 길이가 길지 않아서 그냥 최대값 그대로 진행합니다. (패딩)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce876dd87ee220ea"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def pad_sequences(sentences: [[int]], max_len: int) -> np.ndarray:\n",
    "    features = np.zeros((len(sentences), max_len), dtype=int)\n",
    "    for index, sentence in enumerate(sentences):\n",
    "        if len(sentence) != 0:\n",
    "            features[index, :len(sentence)] = np.array(sentence)[:max_len]\n",
    "    return features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f31851242ee3396a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "padded_X_train = pad_sequences(X_train, max_len=max_len)\n",
    "padded_X_valid = pad_sequences(X_valid, max_len=max_len)\n",
    "padded_X_test = pad_sequences(X_test, max_len=max_len)\n",
    "\n",
    "print('훈련 데이터의 크기 :', padded_X_train.shape)\n",
    "print('검증 데이터의 크기 :', padded_X_valid.shape)\n",
    "print('테스트 데이터의 크기 :', padded_X_test.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1da659db6aef7e03",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "padded_X_test[:5, :]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "859217e5388d9262",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check the GPU models (cuz I use M2 mac and 4080 on pc either)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "abc553c2b7743391"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from utils.device import get_device\n",
    "\n",
    "device = get_device()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "561cae17be0daef1",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Modeling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c6c031b0c35327b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_label_tensor = torch.tensor(np.array(y_train))\n",
    "valid_label_tensor = torch.tensor(np.array(y_valid))\n",
    "test_label_tensor = torch.tensor(np.array(y_test))\n",
    "print(train_label_tensor[:5])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e5f3b9e393d1ee8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class TextCNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, num_labels):\n",
    "        super(TextCNN, self).__init__()\n",
    "\n",
    "        # 오직 하나의 종류의 필터만 사용함.\n",
    "        self.num_filter_sizes = 1  # 윈도우 5짜리 1개만 사용\n",
    "        self.num_filters = 256\n",
    "\n",
    "        self.word_embed = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=128, padding_idx=0)\n",
    "        # 윈도우 5짜리 1개만 사용\n",
    "        self.conv1 = torch.nn.Conv1d(128, self.num_filters, 5, stride=1)\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.fc1 = torch.nn.Linear(1 * self.num_filters, num_labels, bias=True)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # word_embed(inputs).shape == (배치 크기, 문장길이, 임베딩 벡터의 차원)\n",
    "        # word_embed(inputs).permute(0, 2, 1).shape == (배치 크기, 임베딩 벡터의 차원, 문장 길이)\n",
    "        embedded = self.word_embed(inputs).permute(0, 2, 1)\n",
    "\n",
    "        # max를 이용한 maxpooling\n",
    "        # conv1(embedded).shape == (배치 크기, 커널 개수, 컨볼루션 연산 결과) == ex) 32, 256, 496\n",
    "        # conv1(embedded).permute(0, 2, 1).shape == (배치 크기, 컨볼루션 연산 결과, 커널 개수)\n",
    "        # conv1(embedded).permute(0, 2, 1).max(1)[0]).shape == (배치 크기, 커널 개수)\n",
    "        x = F.relu(self.conv1(embedded).permute(0, 2, 1).max(1)[0])\n",
    "\n",
    "        # y_pred.shape == (배치 크기, 분류할 카테고리의 수)\n",
    "        y_pred = self.fc1(self.dropout(x))\n",
    "\n",
    "        return y_pred"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2b6063a61f91949",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "class TextCNNLightning(L.LightningModule):\n",
    "    def __init__(self, vocab_size, num_labels):\n",
    "        super().__init__()\n",
    "\n",
    "        # Parameters\n",
    "        self.num_filter_sizes = 1  # Only using one kind of filter\n",
    "        self.num_filters = 256\n",
    "\n",
    "        # Layers\n",
    "        self.word_embed = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=128, padding_idx=0)\n",
    "        self.conv1 = torch.nn.Conv1d(128, self.num_filters, 5, stride=1)\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.fc1 = torch.nn.Linear(1 * self.num_filters, num_labels, bias=True)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Word embedding\n",
    "        embedded = self.word_embed(inputs).permute(0, 2, 1)\n",
    "\n",
    "        # Convolution and max pooling\n",
    "        x = F.relu(self.conv1(embedded).permute(0, 2, 1).max(1)[0])\n",
    "\n",
    "        # Dropout and fully connected layer\n",
    "        y_pred = self.fc1(self.dropout(x))\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Adam optimizer with default parameters\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Get inputs and labels\n",
    "        inputs, labels = batch\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = self(inputs)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "\n",
    "        # Log loss\n",
    "        self.log(\"train_loss\", loss)\n",
    "\n",
    "        # Return loss\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Get inputs and labels\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = self(inputs)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        acc = (outputs.argmax(dim=1) == labels).float().mean().item() #torch.sum(outputs.argmax(dim=1) == labels) / len(labels) \n",
    "        \"\"\"\n",
    "        def calculate_accuracy(logits, labels):\n",
    "            predicted = torch.argmax(logits, dim=1)\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            total = labels.size(0)\n",
    "            accuracy = correct / total\n",
    "            return accuracy\n",
    "        \"\"\"\n",
    "\n",
    "        # Log loss and accuracy\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.log(\"val_acc\", acc)\n",
    "\n",
    "        # Return loss and accuracy\n",
    "        return loss, acc\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        encoded_train = torch.tensor(padded_X_train).to(torch.int32)\n",
    "        train_dataset = torch.utils.data.TensorDataset(encoded_train, train_label_tensor)\n",
    "        train_dataloader = torch.utils.data.DataLoader(train_dataset, shuffle=True, num_workers=7)\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        encoded_valid = torch.tensor(padded_X_valid).to(torch.int32)\n",
    "        valid_dataset = torch.utils.data.TensorDataset(encoded_valid, valid_label_tensor)\n",
    "        valid_dataloader = torch.utils.data.DataLoader(valid_dataset, shuffle=False, batch_size=1, num_workers=7)\n",
    "        return valid_dataloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        encoded_test = torch.tensor(padded_X_test).to(torch.int32)\n",
    "        test_dataset = torch.utils.data.TensorDataset(encoded_test, test_label_tensor)\n",
    "        test_dataloader = torch.utils.data.DataLoader(test_dataset, shuffle=False, batch_size=1)\n",
    "        return test_dataloader\n",
    "\n",
    "    # def test_step(self, batch, batch_idx):\n",
    "    #     batch_X, batch_y = batch\n",
    "    #     batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)\n",
    "    #     logits = self(batch_X)\n",
    "    #     loss = self.criterion(logits, batch_y)\n",
    "    # \n",
    "    #     # Calculate test accuracy\n",
    "    #     acc = calculate_accuracy(logits, batch_y)\n",
    "    # \n",
    "    #     # Log metrics\n",
    "    #     self.log('test_loss', loss)\n",
    "    #     self.log('test_acc', acc)\n",
    "    # \n",
    "    #     return loss  # Optional but can be used for model selection \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4982a56064326190",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = TextCNNLightning(vocab_size=vocab_size, num_labels=len(set(y_train)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25a386b6247374ec",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b56c15924652389"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# train the model (hint: here are some helpful Trainer arguments for rapid idea iteration)\n",
    "trainer = L.Trainer(max_epochs=1, accelerator=\"auto\", devices=\"auto\", strategy=\"auto\")\n",
    "trainer.fit(model=model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b5662c64c4bcf84",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "trainer.test()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b2f03f95ce98269e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "48e71695ade82ad0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
