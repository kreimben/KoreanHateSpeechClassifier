{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Korean Hate Speech Classifier\n",
    "## Written by: [Jehwan Kim](github.com/kreimben)\n",
    "## Date: 19th Feb 2024\n",
    "## Referenced Paper: \n",
    "* [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1408.5882)\n",
    "* [Cyclical Learning Rates for Training Neural Networks](https://arxiv.org/abs/1506.01186)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b85a589180c7304"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load the word2vec model first,"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76142ca29aea3a03"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95ba83ff602bf2ae",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### And then, load labeled data using pandas"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc778d92bb73c7c1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dev_df = pd.read_csv('./labeled/dev.tsv', sep='\\t')\n",
    "train_df = pd.read_csv('./labeled/train.tsv', sep='\\t')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "483cddf9b1ef8ff5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_df.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "957b5d894af019aa",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dev_df.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6998e4faa8a15643",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### In `hate` column, `offensive`, `none`, `hate`.\n",
    "### In `contain_gender_bias` column, `True`, `False`.\n",
    "### In `bias` column, `none`, `gender`, `others`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f3074856f112f7c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# combine train and dev data.\n",
    "df = pd.concat([dev_df, train_df], ignore_index=True)\n",
    "df.sample(5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57a62ec519832594",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.hate.value_counts(), dev_df.contain_gender_bias.value_counts(), dev_df.bias.value_counts()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "adc2696d1f6809b2",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load words data and tokeniser from past project."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48640b0cad2eddfe"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('tokenizer.pkl', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "with open('words.pkl', 'rb') as handle:\n",
    "    words = pickle.load(handle)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c309aefe176e427a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from utils.stopwords import STOP_WORDS\n",
    "\n",
    "\n",
    "def tokenize(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    preprocess = lambda x: [w for w in x if w not in STOP_WORDS]\n",
    "    return preprocess(tokens)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d3dbea551c4ae89",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sample = df.sample(1).comments.values[0]\n",
    "\n",
    "sample, tokenize(sample)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32c7deac9d238855",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df['tokens'] = df['comments'].apply(tokenize)\n",
    "# 공격적인(offensive) 댓글 또한 혐오 데이터 셋으로 분류함.\n",
    "df['hate'] = df['hate'].replace(['none', 'offensive', 'hate'], [0, 1, 1])\n",
    "df['contain_gender_bias'] = df['contain_gender_bias'].replace([True, False], [1, 0])\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da709590362b1e96",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for sentence in df.tokens:\n",
    "    for word in sentence: vocab.add(word)\n",
    "\n",
    "# TODO: filter the stopwords\n",
    "vocab_size = len(vocab)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b281c2177fdf376",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Vectorise"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7ef7501882fb09f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with open('vectorizer.pkl', 'rb') as handle:\n",
    "    vectorizer = pickle.load(handle)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b6238857638d8c2",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Encoding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9a0973672e14a64"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df['encoding'] = df['comments'].apply(vectorizer.encode_a_doc_to_list)\n",
    "df.encoding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae2f4030b420969e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_data = df['encoding']\n",
    "y_data = df['hate']\n",
    "print(y_data.value_counts())\n",
    "len(X_data), len(y_data), len(X_data) == len(y_data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "abc1b33996c85487",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=.1, random_state=0, stratify=y_data)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=.1, random_state=0, stratify=y_train)\n",
    "\n",
    "y_train.value_counts()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aea73a715225d891",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print('공격적인(offensive) 댓글 또한 혐오 데이터 셋으로 분류함.')\n",
    "print('--------훈련 데이터의 비율-----------')\n",
    "print(f'혐오 댓글 = {round(y_train.value_counts()[1] / len(y_train) * 100, 3)}%')\n",
    "print(f'일반 댓글 = {round(y_train.value_counts()[0] / len(y_train) * 100, 3)}%')\n",
    "print('--------검증 데이터의 비율-----------')\n",
    "print(f'혐오 댓글 = {round(y_valid.value_counts()[1] / len(y_valid) * 100, 3)}%')\n",
    "print(f'일반 댓글 = {round(y_valid.value_counts()[0] / len(y_valid) * 100, 3)}%')\n",
    "print('--------테스트 데이터의 비율-----------')\n",
    "print(f'혐오 댓글 = {round(y_test.value_counts()[1] / len(y_test) * 100, 3)}%')\n",
    "print(f'일반 댓글 = {round(y_test.value_counts()[0] / len(y_test) * 100, 3)}%')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "636c107f5616018a",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Padding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc3cc8cad19abafc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print('댓글의 최대 길이 :', max(len(review) for review in X_train))\n",
    "print('댓글의 평균 길이 :', sum(map(len, X_train)) / len(X_train))\n",
    "plt.hist([len(review) for review in X_train], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9771b28e6e289d0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "max_len = max(len(review) for review in X_train)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e299ce12a3f55b8",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 길이가 길지 않아서 그냥 최대값 그대로 진행합니다. (패딩)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce876dd87ee220ea"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def pad_sequences(sentences: [[int]], max_len: int) -> np.ndarray:\n",
    "    features = np.zeros((len(sentences), max_len), dtype=int)\n",
    "    for index, sentence in enumerate(sentences):\n",
    "        if len(sentence) != 0:\n",
    "            features[index, :len(sentence)] = np.array(sentence)[:max_len]\n",
    "    return features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f31851242ee3396a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "padded_X_train = pad_sequences(X_train, max_len=max_len)\n",
    "padded_X_valid = pad_sequences(X_valid, max_len=max_len)\n",
    "padded_X_test = pad_sequences(X_test, max_len=max_len)\n",
    "\n",
    "print('훈련 데이터의 크기 :', padded_X_train.shape)\n",
    "print('검증 데이터의 크기 :', padded_X_valid.shape)\n",
    "print('테스트 데이터의 크기 :', padded_X_test.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1da659db6aef7e03",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "padded_X_test[:5, :]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "859217e5388d9262",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check the GPU models (cuz I use M2 mac and 4080 on pc either)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "abc553c2b7743391"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from utils.device import get_device\n",
    "\n",
    "device = get_device()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "561cae17be0daef1",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Modeling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c6c031b0c35327b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_label_tensor = torch.tensor(np.array(y_train))\n",
    "valid_label_tensor = torch.tensor(np.array(y_valid))\n",
    "test_label_tensor = torch.tensor(np.array(y_test))\n",
    "print(train_label_tensor[:5])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e5f3b9e393d1ee8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class TextCNNLightning(L.LightningModule):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout, train_batch_size):\n",
    "        super().__init__()\n",
    "        self.lr = None\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(fs, embedding_dim))\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # text = [batch size, sent len]\n",
    "        embedded = self.embedding(text)  # embedded = [batch size, sent len, emb dim]\n",
    "        embedded = embedded.unsqueeze(1)  # embedded = [batch size, 1, sent len, emb dim]\n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "        # conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        # pooled_n = [batch size, n_filters]\n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "        # cat = [batch size, n_filters * len(filter_sizes)]\n",
    "        return self.fc(cat)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters())\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Get inputs and labels\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = self(inputs)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "\n",
    "        # Log loss\n",
    "        self.log(\"train_loss\", loss)\n",
    "\n",
    "        # Return loss\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Get inputs and labels\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = self(inputs)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        acc = (outputs.argmax(dim=1) == labels).float().mean().item()\n",
    "\n",
    "        # Log loss and accuracy\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.log(\"val_acc\", acc)\n",
    "\n",
    "        # Return loss and accuracy\n",
    "        return loss, acc\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Get inputs and labels\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = self(inputs)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        acc = (outputs.argmax(dim=1) == labels).float().mean().item()\n",
    "\n",
    "        # Log loss and accuracy\n",
    "        self.log(\"test_loss\", loss)\n",
    "        self.log(\"test_acc\", acc)\n",
    "\n",
    "        # Return loss and accuracy\n",
    "        return loss, acc\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        encoded_train = torch.tensor(padded_X_train).to(torch.int32)\n",
    "        train_dataset = torch.utils.data.TensorDataset(encoded_train, train_label_tensor)\n",
    "        train_dataloader = torch.utils.data.DataLoader(train_dataset, shuffle=True, num_workers=7,\n",
    "                                                       persistent_workers=True, batch_size=self.train_batch_size)\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        encoded_valid = torch.tensor(padded_X_valid).to(torch.int32)\n",
    "        valid_dataset = torch.utils.data.TensorDataset(encoded_valid, valid_label_tensor)\n",
    "        valid_dataloader = torch.utils.data.DataLoader(valid_dataset, shuffle=False, batch_size=1, num_workers=7,\n",
    "                                                       persistent_workers=True)\n",
    "        return valid_dataloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        encoded_test = torch.tensor(padded_X_test).to(torch.int32)\n",
    "        test_dataset = torch.utils.data.TensorDataset(encoded_test, test_label_tensor)\n",
    "        test_dataloader = torch.utils.data.DataLoader(test_dataset, shuffle=False, batch_size=1, num_workers=7)\n",
    "        return test_dataloader"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4982a56064326190",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# model = TextCNNLightning(vocab_size=vocab_size, num_labels=len(set(y_train)), l=l)\n",
    "model = TextCNNLightning(vocab_size=vocab_size, embedding_dim=300, n_filters=100,\n",
    "                         filter_sizes=[3, 4, 5], output_dim=2, dropout=.5, train_batch_size=512)\n",
    "model.to(device)\n",
    "\n",
    "vocab_size, len(set(y_train))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25a386b6247374ec",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b56c15924652389"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from lightning.pytorch.callbacks import LearningRateFinder\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    accelerator=\"auto\", devices=\"auto\", strategy=\"auto\",\n",
    "    max_epochs=30, callbacks=[LearningRateFinder()]\n",
    ")\n",
    "trainer.fit(model=model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b5662c64c4bcf84",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "trainer.test()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b2f03f95ce98269e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# HPO using optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the hyperparameter space\n",
    "    embedding_dim = trial.suggest_int(\"embedding_dim\", 100, 500)\n",
    "    n_filters = trial.suggest_int(\"n_filters\", 100, 300)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', .1, .9)\n",
    "    \n",
    "    # Suggest a logarithmic value\n",
    "    log_base_2_value = trial.suggest_int('log_base_2_value', 0, 10)\n",
    "    # Convert to actual value\n",
    "    train_batch_size = 2 ** log_base_2_value\n",
    "\n",
    "    # Initialize the model with the hyperparameters\n",
    "    model = TextCNNLightning(vocab_size=vocab_size, embedding_dim=embedding_dim, n_filters=n_filters,\n",
    "                             filter_sizes=[3, 4, 5], output_dim=2, dropout=dropout_rate,\n",
    "                             train_batch_size=train_batch_size)\n",
    "\n",
    "    # Trainer settings\n",
    "    trainer = L.Trainer(\n",
    "        accelerator=\"auto\", devices=\"auto\", strategy=\"auto\", max_epochs=10\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.fit(model)\n",
    "\n",
    "    # Evaluate the model performance\n",
    "    return trainer.callback_metrics[\"train_loss\"]\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48e71695ade82ad0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "# Create a study object\n",
    "study = optuna.create_study()  # or 'maximize' based on your goal\n",
    "study.optimize(objective, n_trials=100)  # Specify the number of trials\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(f\"Best trial: {study.best_trial.params}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "72974d1d37e941f3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "41ca4069c519ff53"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
