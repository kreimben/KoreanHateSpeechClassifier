{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Korean Hate Speech Classifier\n",
    "## Written by: [Jehwan Kim](github.com/kreimben)\n",
    "## Date: 19th Feb 2024\n",
    "## Referenced Paper: \n",
    "* [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1408.5882)\n",
    "* [Cyclical Learning Rates for Training Neural Networks](https://arxiv.org/abs/1506.01186)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b85a589180c7304"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load the word2vec model first,"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76142ca29aea3a03"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95ba83ff602bf2ae",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### And then, load labeled data using pandas"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc778d92bb73c7c1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dev_df = pd.read_csv('./labeled/dev.tsv', sep='\\t')\n",
    "train_df = pd.read_csv('./labeled/train.tsv', sep='\\t')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "483cddf9b1ef8ff5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_df.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "957b5d894af019aa",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dev_df.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6998e4faa8a15643",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### In `hate` column, `offensive`, `none`, `hate`.\n",
    "### In `contain_gender_bias` column, `True`, `False`.\n",
    "### In `bias` column, `none`, `gender`, `others`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f3074856f112f7c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# combine train and dev data.\n",
    "df = pd.concat([dev_df, train_df], ignore_index=True)\n",
    "df.sample(5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57a62ec519832594",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.hate.value_counts(), dev_df.contain_gender_bias.value_counts(), dev_df.bias.value_counts()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "adc2696d1f6809b2",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load words data and tokeniser from past project."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48640b0cad2eddfe"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('tokenizer.pkl', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "with open('words.pkl', 'rb') as handle:\n",
    "    words = pickle.load(handle)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c309aefe176e427a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def tokenize(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    preprocess = lambda x: [w for w in x if w not in STOP_WORDS]\n",
    "    return preprocess(tokens)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d3dbea551c4ae89",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sample = df.sample(1).comments.values[0]\n",
    "\n",
    "sample, tokenize(sample)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32c7deac9d238855",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df['tokens'] = df['comments'].apply(tokenize)\n",
    "# 공격적인(offensive) 댓글 또한 혐오 데이터 셋으로 분류함.\n",
    "df['hate'] = df['hate'].replace(['none', 'offensive', 'hate'], [0, 1, 1])\n",
    "df['contain_gender_bias'] = df['contain_gender_bias'].replace([True, False], [1, 0])\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da709590362b1e96",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for sentence in df.tokens:\n",
    "    for word in sentence: vocab.add(word)\n",
    "\n",
    "vocab_size = len(vocab)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b281c2177fdf376",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Vectorise"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7ef7501882fb09f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with open('vectorizer.pkl', 'rb') as handle:\n",
    "    vectorizer = pickle.load(handle)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b6238857638d8c2",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Encoding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9a0973672e14a64"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df['encoding'] = df['comments'].apply(vectorizer.encode_a_doc_to_list)\n",
    "df.encoding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae2f4030b420969e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_data = df['encoding']\n",
    "y_data = df['hate']\n",
    "print(y_data.value_counts())\n",
    "len(X_data), len(y_data), len(X_data) == len(y_data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "abc1b33996c85487",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=.1, random_state=0, stratify=y_data)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=.1, random_state=0, stratify=y_train)\n",
    "\n",
    "y_train.value_counts()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aea73a715225d891",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print('공격적인(offensive) 댓글 또한 혐오 데이터 셋으로 분류함.')\n",
    "print('--------훈련 데이터의 비율-----------')\n",
    "print(f'혐오 댓글 = {round(y_train.value_counts()[1] / len(y_train) * 100, 3)}%')\n",
    "print(f'일반 댓글 = {round(y_train.value_counts()[0] / len(y_train) * 100, 3)}%')\n",
    "print('--------검증 데이터의 비율-----------')\n",
    "print(f'혐오 댓글 = {round(y_valid.value_counts()[1] / len(y_valid) * 100, 3)}%')\n",
    "print(f'일반 댓글 = {round(y_valid.value_counts()[0] / len(y_valid) * 100, 3)}%')\n",
    "print('--------테스트 데이터의 비율-----------')\n",
    "print(f'혐오 댓글 = {round(y_test.value_counts()[1] / len(y_test) * 100, 3)}%')\n",
    "print(f'일반 댓글 = {round(y_test.value_counts()[0] / len(y_test) * 100, 3)}%')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "636c107f5616018a",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Padding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc3cc8cad19abafc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print('댓글의 최대 길이 :', max(len(review) for review in X_train))\n",
    "print('댓글의 평균 길이 :', sum(map(len, X_train)) / len(X_train))\n",
    "plt.hist([len(review) for review in X_train], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9771b28e6e289d0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "max_len = max(len(review) for review in X_train)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e299ce12a3f55b8",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 길이가 길지 않아서 그냥 최대값 그대로 진행합니다. (패딩)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce876dd87ee220ea"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def pad_sequences(sentences: [[int]], max_len: int) -> np.ndarray:\n",
    "    features = np.zeros((len(sentences), max_len), dtype=int)\n",
    "    for index, sentence in enumerate(sentences):\n",
    "        if len(sentence) != 0:\n",
    "            features[index, :len(sentence)] = np.array(sentence)[:max_len]\n",
    "    return features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f31851242ee3396a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "padded_X_train = pad_sequences(X_train, max_len=max_len)\n",
    "padded_X_valid = pad_sequences(X_valid, max_len=max_len)\n",
    "padded_X_test = pad_sequences(X_test, max_len=max_len)\n",
    "\n",
    "print('훈련 데이터의 크기 :', padded_X_train.shape)\n",
    "print('검증 데이터의 크기 :', padded_X_valid.shape)\n",
    "print('테스트 데이터의 크기 :', padded_X_test.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1da659db6aef7e03",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "padded_X_test[:5, :]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "859217e5388d9262",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Modeling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c6c031b0c35327b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_label_tensor = torch.tensor(np.array(y_train))\n",
    "valid_label_tensor = torch.tensor(np.array(y_valid))\n",
    "test_label_tensor = torch.tensor(np.array(y_test))\n",
    "print(train_label_tensor[:5])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e5f3b9e393d1ee8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class TextCNNLightning(L.LightningModule):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout, train_batch_size):\n",
    "        super().__init__()\n",
    "        self.lr = None\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(fs, embedding_dim))\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # text = [batch size, sent len]\n",
    "        embedded = self.embedding(text)  # embedded = [batch size, sent len, emb dim]\n",
    "        embedded = embedded.unsqueeze(1)  # embedded = [batch size, 1, sent len, emb dim]\n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "        # conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        # pooled_n = [batch size, n_filters]\n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "        # cat = [batch size, n_filters * len(filter_sizes)]\n",
    "        return self.fc(cat)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters())\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Get inputs and labels\n",
    "        inputs, labels = batch\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = self(inputs)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "\n",
    "        # Log loss\n",
    "        self.log(\"train_loss\", loss)\n",
    "\n",
    "        # Return loss\n",
    "        return loss\n",
    "\n",
    "    def __accuracy(self, outputs, labels):\n",
    "        predictions = outputs.argmax(dim=1)  # Get indices of highest probability\n",
    "        correct = (predictions == labels).sum().item()\n",
    "        acc = correct / len(labels)\n",
    "        return acc\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Get inputs and labels\n",
    "        inputs, labels = batch\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = self(inputs)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        acc = self.__accuracy(outputs, labels)\n",
    "\n",
    "        # Log loss and accuracy\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.log(\"val_acc\", acc)\n",
    "\n",
    "        # Return loss and accuracy\n",
    "        return loss, acc\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Get inputs and labels\n",
    "        inputs, labels = batch\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = self(inputs)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        acc = self.__accuracy(outputs, labels)\n",
    "\n",
    "        # Log loss and accuracy\n",
    "        self.log(\"test_loss\", loss)\n",
    "        self.log(\"test_acc\", acc)\n",
    "\n",
    "        # Return loss and accuracy\n",
    "        return loss, acc\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        encoded_train = torch.tensor(padded_X_train).to(torch.int32)\n",
    "        train_dataset = TensorDataset(encoded_train, train_label_tensor)\n",
    "        train_dataloader = DataLoader(train_dataset, shuffle=True, num_workers=7,\n",
    "                                      persistent_workers=True, batch_size=self.train_batch_size)\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        encoded_valid = torch.tensor(padded_X_valid).to(torch.int32)\n",
    "        valid_dataset = TensorDataset(encoded_valid, valid_label_tensor)\n",
    "        valid_dataloader = DataLoader(valid_dataset, shuffle=False, batch_size=1, num_workers=7,\n",
    "                                      persistent_workers=True)\n",
    "        return valid_dataloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        encoded_test = torch.tensor(padded_X_test).to(torch.int32)\n",
    "        test_dataset = TensorDataset(encoded_test, test_label_tensor)\n",
    "        test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=1, num_workers=7)\n",
    "        return test_dataloader"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4982a56064326190",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # HPO using optuna\n",
    "# \n",
    "# def objective(trial):\n",
    "#     # Define the hyperparameter space\n",
    "#     embedding_dim = trial.suggest_int(\"embedding_dim\", 100, 500)\n",
    "#     n_filters = trial.suggest_int(\"n_filters\", 100, 300)\n",
    "#     dropout_rate = trial.suggest_float('dropout_rate', .1, .9)\n",
    "# \n",
    "#     # Suggest a logarithmic value\n",
    "#     log_base_2_value = trial.suggest_int('log_base_2_value', 0, 10)\n",
    "#     # Convert to actual value\n",
    "#     train_batch_size = 2 ** log_base_2_value\n",
    "# \n",
    "#     # Initialize the model with the hyperparameters\n",
    "#     model = TextCNNLightning(vocab_size=vocab_size, embedding_dim=embedding_dim, n_filters=n_filters,\n",
    "#                              filter_sizes=[3, 4, 5], output_dim=2, dropout=dropout_rate,\n",
    "#                              train_batch_size=train_batch_size)\n",
    "# \n",
    "#     # Trainer settings\n",
    "#     trainer = L.Trainer(\n",
    "#         accelerator=\"auto\", max_epochs=15\n",
    "#     )\n",
    "# \n",
    "#     # Train the model\n",
    "#     trainer.fit(model)\n",
    "# \n",
    "#     # Evaluate the model performance\n",
    "#     return trainer.callback_metrics[\"train_loss\"]\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48e71695ade82ad0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# \n",
    "# # Create a study object\n",
    "# study = optuna.create_study()  # or 'maximize' based on your goal\n",
    "# study.optimize(objective, n_trials=100)  # Specify the number of trials\n",
    "# \n",
    "# # Print the best hyperparameters\n",
    "# print(f\"Best trial: {study.best_trial.params}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72974d1d37e941f3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# model = TextCNNLightning(vocab_size=vocab_size, num_labels=len(set(y_train)), l=l)\n",
    "model = TextCNNLightning(vocab_size=vocab_size, embedding_dim=300, n_filters=100,\n",
    "                         filter_sizes=[3, 4, 5], output_dim=2, dropout=.5, train_batch_size=512)\n",
    "\n",
    "vocab_size, len(set(y_train))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25a386b6247374ec",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b56c15924652389"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from lightning.pytorch.callbacks import LearningRateFinder\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    accelerator=\"auto\", devices=\"auto\", strategy=\"auto\",\n",
    "    max_epochs=10, callbacks=[LearningRateFinder()]\n",
    ")\n",
    "trainer.fit(model=model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b5662c64c4bcf84",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "trainer.test()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b2f03f95ce98269e",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using Bert to classificate"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79089b972fa80692"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from icecream import ic\n",
    "from utils.evaluate import calculate_accuracy\n",
    "from transformers import BertModel\n",
    "\n",
    "\n",
    "class BertClassifier(L.LightningModule):\n",
    "    def __init__(self, n_classes: int, pretrained_model_name=\"klue/bert-base\", steps_per_epoch=None, n_epochs=None, batch_size=32):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.bert = BertModel.from_pretrained(pretrained_model_name)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.n_epochs = n_epochs\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return self.classifier(output.pooler_output)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        \n",
    "        input_ids = input_ids.reshape(-1, input_ids.shape[-1])  # Keep last dimension dynamic\n",
    "        attention_mask = attention_mask.reshape(-1, attention_mask.shape[-1])\n",
    "        \n",
    "        outputs = self(input_ids, attention_mask)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Get inputs and labels\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        \n",
    "        input_ids = input_ids.reshape(-1, input_ids.shape[-1])  # Keep last dimension dynamic\n",
    "        attention_mask = attention_mask.reshape(-1, attention_mask.shape[-1])\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = self(input_ids, attention_mask)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        acc = calculate_accuracy(outputs, labels)\n",
    "\n",
    "        # Log loss and accuracy\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.log(\"val_acc\", acc)\n",
    "\n",
    "        # Return loss and accuracy\n",
    "        return loss, acc\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # Get inputs and labels\n",
    "        input_ids, attention_mask, labels = batch\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = self(input_ids, attention_mask)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        acc = calculate_accuracy(outputs, labels)\n",
    "\n",
    "        # Log loss and accuracy\n",
    "        self.log(\"test_loss\", loss)\n",
    "        self.log(\"test_acc\", acc)\n",
    "\n",
    "        # Return loss and accuracy\n",
    "        return loss, acc\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=2e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=self.steps_per_epoch * 0.1, gamma=0.1)\n",
    "        return [optimizer], [scheduler]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f55b3cbe769624ff",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from utils.stopwords import STOP_WORDS\n",
    "\n",
    "dev_df = pd.read_csv('./labeled/dev.tsv', sep='\\t')\n",
    "train_df = pd.read_csv('./labeled/train.tsv', sep='\\t')\n",
    "df = pd.concat([dev_df, train_df], ignore_index=True)\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "bert_model = BertModel.from_pretrained(\"klue/bert-base\")\n",
    "tokenizer = BertTokenizer.from_pretrained('klue/bert-base')\n",
    "\n",
    "tokenized_batch = tokenizer.batch_encode_plus(df.comments.values, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "input_ids = tokenized_batch['input_ids']\n",
    "attention_mask = tokenized_batch['attention_mask']\n",
    "y_label_tensor = torch.tensor(np.array(y_data))\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_mask, y_label_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size])\n",
    "train_dataloader, val_dataloader, test_dataloader = DataLoader(train_set, batch_size=32, shuffle=True), DataLoader(val_set, batch_size=32), DataLoader(test_set, batch_size=32)\n",
    "\n",
    "vocab_size = tokenizer.vocab_size"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6522087995a23c9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 모델 인스턴스화 및 훈련 파라미터 설정\n",
    "bert_classifier_model = BertClassifier(n_classes=2, steps_per_epoch=100, n_epochs=3)\n",
    "\n",
    "# 트레이너 설정 및 훈련 시작\n",
    "bert_trainer = L.Trainer(max_epochs=3)\n",
    "bert_trainer.fit(bert_classifier_model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6355ae9da6e7bf6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "bert_trainer.test(dataloaders=[test_dataloader])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89c5b202046732a2",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 최종 결과\n",
    "\n",
    "* 이전 프로젝트인 word2vec에서 직접 가져온 tokeniser로 단어 임베딩을 진행한 결과 \n",
    "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
    "       Test metric             DataLoader 0\n",
    "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
    "        test_acc            0.6463560461997986\n",
    "        test_loss           0.7093809247016907\n",
    "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
    "정확도는 64%이고 손실률은 0.7이 됐다.\n",
    "손수 직접 임베딩을 한 것이라, 아무리 epoch를 더 돌린다 해도 정확도가 더 올라가지 않아 포기했다.\n",
    "HPO 관련 라이브러리를 설치해봤으나 시간이 너무 부족해 hpo는 하지 못했다.\n",
    "\n",
    "* bert에서 가져온 tokeniser로 단어 임베딜을 진행한 결과\n",
    "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
    "       Test metric             DataLoader 0\n",
    "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
    "        test_acc            0.7778662443161011\n",
    "        test_loss           0.7982401847839355\n",
    "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
    "정확도는 77%이고 손실률은 0.79이다.\n",
    "\n",
    "---\n",
    "\n",
    "프로젝트를 진행하면서 의문이였던 점은, 'tokeniser의 차이로 문장들의 전체 길이가 달라진다는 점이 최종 결과까지 영향을 줄 수 있는가'이다.\n",
    "또한 '이모티콘이나 자음을 이용해 ㅇㅁㅇ 이나 (ㅇㅅㅇ) 와 같은 것을 쓴다면 이 역시 stopword나 정규식을 이용해 필터링 해야하는가'이다.\n",
    "한국인들은 ㅋ 와 ㅋㅋ 의 의미를 다르게 받아들인다. 이럴 경우 tokenising 처리를 어디까지 해줘야 하는지가 관건인것 같다. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f07e7381a1b87b9f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
